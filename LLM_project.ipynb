{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408eac16-d744-454b-bf02-6b4f92716e96",
   "metadata": {},
   "source": [
    "### Why This Project Stands Out\n",
    "- Real-World Dataset: Scrapes real academic papers from arXiv.\n",
    "- State-of-the-Art Tools: Combines BERT for ranking and LLM for summarization.\n",
    "- Relevance to NLP: Covers embedding, retrieval, and question-answering pipelines.\n",
    "- Practical Impact: Useful for researchers and students looking for summarized academic insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9511cba-c6e5-4807-869c-ead8467143d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to arxiv_papers.csv!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/jwsf6hg14r9gd574_gfnfl940000gn/T/ipykernel_64492/4106052161.py:16: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(response.content, \"lxml\")\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Scrape research paper abstracts from arXiv\n",
    "def scrape_arxiv(query, max_results=10):\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{query}\",\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        papers = []\n",
    "        for entry in soup.find_all(\"entry\"):\n",
    "            title = entry.find(\"title\").text\n",
    "            summary = entry.find(\"summary\").text\n",
    "            papers.append({\"title\": title.strip(), \"abstract\": summary.strip()})\n",
    "        return pd.DataFrame(papers)\n",
    "    else:\n",
    "        raise Exception(\"Failed to fetch data from arXiv API\")\n",
    "\n",
    "# Step 2: Save dataset to CSV\n",
    "query = \"machine learning\"\n",
    "papers_df = scrape_arxiv(query, max_results=20)\n",
    "papers_df.to_csv(\"arxiv_papers.csv\", index=False)\n",
    "print(\"Dataset saved to arxiv_papers.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77fbcaed-3009-4341-8eaa-eb829bbb9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers_df = pd.read_csv(\"arxiv_papers.csv\")\n",
    "# papers = papers_df[\"abstract\"].tolist()\n",
    "# papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43347d9f-b2a9-491f-868d-f79a91cc7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeee01aa-0cfb-4a52-9dad-240aa65fef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking papers with BERT...\n",
      "\n",
      "Top Papers and Summaries:\n",
      "\n",
      "Paper 1:\n",
      "Abstract: Transparent machine learning is introduced as an alternative form of machine\n",
      "learning, where both the model and the learning system are represented in\n",
      "source code form. The goal of this project is to enable direct human\n",
      "understanding of machine learning models, giving us the ability to learn,\n",
      "verify, and refine them as programs. If solved, this technology could represent\n",
      "a best-case scenario for the safety and security of AI systems going forward.\n",
      "Summary: Here is a summary of the paper's abstract in a few sentences:\n",
      "\n",
      "This paper introduces transparent machine learning, where both models and learning systems are represented in source code form to enable human understanding and verification. The goal is to improve the safety and security of AI systems by making them more transparent and explainable.\n",
      "\n",
      "Paper 2:\n",
      "Abstract: Introduction to Machine learning covering Statistical Inference (Bayes, EM,\n",
      "ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\n",
      "and PAC learning (the Formal model, VC dimension, Double Sampling theorem).\n",
      "Summary: I don't know the answer to this question as it appears to be about a specific paper or research, but I can summarize the provided context for you:\n",
      "\n",
      "This paper introduces and discusses several topics related to machine learning, including statistical inference, algebraic and spectral methods, PAC learning, and more. It then focuses on addressing challenges in machine learning with limited learning samples and proposes a framework called AutoCompete, which aims to automate certain steps of building predictive models and improve performance.\n",
      "\n",
      "Paper 3:\n",
      "Abstract: We take a closer look at some theoretical challenges of Machine Learning as a\n",
      "function approximation, gradient descent as the default optimization algorithm,\n",
      "limitations of fixed length and width networks and a different approach to RNNs\n",
      "from a mathematical perspective.\n",
      "Summary: I don't have enough context to summarize this paper's abstract. The provided text appears to be an introduction to the challenges and limitations of machine learning, but it does not mention the specific topic or focus of the paper being discussed.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load dataset\n",
    "papers_df = pd.read_csv(\"arxiv_papers.csv\")\n",
    "papers = papers_df[\"abstract\"].tolist()\n",
    "\n",
    "# Step 1: Embed documents with HuggingFace\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"allenai/scibert_scivocab_uncased\")\n",
    "vectorstore = FAISS.from_texts(papers, embeddings)\n",
    "\n",
    "# Step 2: Set up LangChain pipeline with an LLM\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_pipeline = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# Step 3: Ranking using BERT (Optional)\n",
    "bert_model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n",
    "\n",
    "def rank_papers(query, papers):\n",
    "    print(\"Ranking papers with BERT...\")\n",
    "    scores = []\n",
    "    max_length = 512  # Set max length to avoid truncation issues\n",
    "    for paper in papers:\n",
    "        # Tokenize the inputs with truncation and padding\n",
    "        inputs = bert_tokenizer.encode_plus(\n",
    "            query, paper, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = bert_model(**inputs)\n",
    "        \n",
    "        # Apply softmax to logits (optional if it's a multi-class classification)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "        score = probs[0][0].item()  # Assuming the first class is the most relevant\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    # Sort papers by scores (higher score is better)\n",
    "    return sorted(zip(scores, papers), reverse=True)\n",
    "\n",
    "# Query the system\n",
    "query = \"Computer\"\n",
    "retrieved_papers = retriever.get_relevant_documents(query)\n",
    "ranked_papers = rank_papers(query, [doc.page_content for doc in retrieved_papers])\n",
    "\n",
    "# Step 4: Summarize the top-ranked papers\n",
    "print(\"\\nTop Papers and Summaries:\")\n",
    "summary_question = \"Please summarize this paper's abstract in a few sentences.\"  # Explicit question for summarization\n",
    "\n",
    "for i, paper in enumerate(ranked_papers[:3], 1):\n",
    "    print(f\"\\nPaper {i}:\")\n",
    "    print(\"Abstract:\", paper[1])\n",
    "    summary = qa_pipeline.run(f\"{summary_question} {paper[1]}\")\n",
    "    print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114c42e-9e92-4da9-bd01-14e89bd2e587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
